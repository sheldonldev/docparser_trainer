# 高效微调

- [Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning]<https://arxiv.org/abs/2303.15647>

## BitFit

只训练 bias

## Prompt-Tuning

冻结模型全部参数，在训练数据前加入一小段 Prompt, 只训练 Prompt 的表示层（即一个 Embedding 模块）。其中 Prompt 有存在两种形式：

- hard prompt: 仅使用一个固定的 Prompt

- soft prompt: 自己学习 Prompt

## P-Tuning

在 Prompt-Tuning 的基础上，对Prompt部分进行进一步编码计算，加速收敛。支持两种编码方式：LSTM 和 MLP。只支持对 Soft Prompt 进行编码。

## Prefix-Tuning

不再将Prompt加在输入的embedding层，而是将其作为可学习的前缀，发在Transformer模型的每一层中，具体表现形式为 past_key_values。

- past_key_values: Transformer模型历史计算过的 key,value 的结果，最早用于生成类模型解码加速，解码逻辑是根据历史输入，每次预测一个新的token，然后将新的token加入输入，再预测下一个token，这个过程会产生大量重复的计算，因此可以将 key, value 的结果保存下来,作为past_key_values输入到下一次计算中，这技术又被称之为kv_cache。

## IA3

Infused Adapter by Inhibiting and Amplifying Inner Activations

抑制和放大内部激活, 通过可学习的向量对激活值进行抑制或放大。具体来说，会对K,V,FFN三部分的值进行调整.训练完成后类似lora进行merge，没有额外推理开销。

通过非常少的参数调节，可以达到非常显著的效果（波动也很大,可以降低学习率）。

## PEFT 进阶操作

### 自定义模型适配器

为自定义的模型适配高效微调

### 多适配器加载与切换

一个主模型，多个适配器

### 禁用适配器

获取原始模型输出
